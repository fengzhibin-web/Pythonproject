# 图像关系图构建设计文档
将CIFAR - 10图像数据转换为图数据，并用于后续图神经网络模型的训练和分类检验的具体流程如下：


### 方案一：基于图像语义关系图，利用百度AI图像识别API提取语义特征，根据相似度构建图，节点为图像，边为语义关联。
### 具体步骤如下：
### 1.  数据加载与预处理
从CIFAR - 10数据集中加载图像数据
将图像数据从原始格式(3, 32, 32)转换为(32, 32, 3)的RGB格式
对图像进行resize到64x64大小，并转换为JPEG格式的base64编码

### 2. 语义特征提取
使用百度AI的图像识别API的子接口**通用物体和场景识别接口**获取图像的高级语义特征
对API返回结果进行筛选，保留置信度高于阈值(MIN_CONFIDENCE)的结果
提取关键词和根类别信息，构建语义特征向量

### 3. 特征工程
将原始图像数据归一化到[0,1]范围
构建语义特征矩阵和置信度矩阵
将图像特征、语义特征和置信度特征拼接成节点特征矩阵

### 4. 图结构构建
1. 计算图像之间的Jaccard相似度矩阵： 
```python
similarity_matrix = np.zeros((NUM_IMAGES, NUM_IMAGES))
for i in range(NUM_IMAGES):
    for j in range(NUM_IMAGES):
        if i != j:
            similarity_matrix[i][j] = jaccard_similarity(
                set(semantic_features[i]),
                set(semantic_features[j])
            )
```
- 使用Jaccard相似度计算每对图像之间的语义相似度
- 比较两幅图像的语义特征集合（关键词集合）
- 计算交集与并集的比例作为相似度值
- 排除自相似度（i == j的情况）  
2. 确定相似度阈值：
```python
similarity_threshold = np.percentile(similarity_matrix, SIMILARITY_PERCENTILE)
```
- 使用百分位数方法确定动态阈值
- 默认使用60%分位数作为基准阈值
使用动态阈值（基于百分位数和固定阈值混合）确定边连接
3. 构建双向边列表和对应的边权重
```python
edges, edge_weights = [], []
for i in range(NUM_IMAGES):
    for j in range(i + 1, NUM_IMAGES):
        if similarity_matrix[i][j] > max(similarity_threshold, 0.25):  # 混合阈值
            edges.extend([(i, j), (j, i)])
            edge_weights.extend([similarity_matrix[i][j]] * 2)
```
- 遍历所有图像对
- 使用混合阈值策略：取动态阈值和固定阈值（0.25）中的较大值
- 如果相似度超过阈值，则建立双向边
- 同时记录对应的边权重（即相似度值）  

#### 这种边构建策略的特点：

1. 基于语义相似度，能够捕捉图像之间的高级语义关系
2. 使用动态阈值，可以适应不同数据集的分布特点
3. 混合阈值策略保证了图的连通性
4. 双向边设计使得图结构更适合后续的图神经网络处理  
最终生成的边列表和边权重将用于构建图的邻接矩阵或边索引，为后续的图神经网络训练提供必要的数据结构。

### 5. 数据保存
将节点特征、边列表和边权重保存为.npz文件

#### 大致流程如下：
```python
# 数据加载
dataset_path = "c:/Users/13525/Desktop/cifar-10-batches-py/data_batch_1"
batch = unpickle(dataset_path)
data, labels = batch[b'data'], batch[b'labels']

# 图像预处理与特征提取
for i in range(NUM_IMAGES):
    image_data = data[i].reshape(3, 32, 32).transpose(1, 2, 0)
    img = Image.fromarray(image_data, 'RGB').resize((64, 64))
    # 转换为base64并调用API获取语义特征

# 特征工程
node_feat = np.hstack([img_features, semantic_feat, confidence_feat]).astype(np.float32)

# 图结构构建
similarity_matrix = np.zeros((NUM_IMAGES, NUM_IMAGES))
for i in range(NUM_IMAGES):
    for j in range(NUM_IMAGES):
        similarity_matrix[i][j] = jaccard_similarity(
            set(semantic_features[i]),
            set(semantic_features[j])
        )

# 数据保存
np.savez('graph_data_optimized.npz',
         node_feat=node_feat,
         edges=np.array(edges, dtype=np.int64),
         edge_weights=np.array(edge_weights, dtype=np.float32))
```
### 方案二：基于图像特征关系图，提取局部特征（如SIFT、HOG），构建图结构，节点为特征，边为空间或关联关系。  
### 具体步骤如下：

### 1. 数据加载与预处理
1. 从CIFAR - 10数据集中加载图像数据
2. 将图像数据从NCHW格式转换为NHWC格式（32x32 RGB图像）
3. 提取前500张图像作为训练集，501 - 1000张作为分类测试集，用于模型训练后的性能评估

### 2. 特征提取
1. 对每张图像进行SIFT特征提取
2. 将SIFT特征与空间位置信息（x,y坐标）结合，形成增强特征
3. 记录每个特征点对应的图像索引（node_image_map）

### 3. 图结构构建
#### 1. 网格划分
- 将32x32的图像划分为8x8的网格，每个网格大小为4x4像素
- 通过cell_size = image_size // grid_size计算每个网格的大小

#### 2. 特征点分配
- 遍历所有特征点，根据其(x,y)坐标将其分配到对应的网格中
- 使用字典grid存储每个网格中的特征点索引

#### 3. 边构建规则
- 同一网格内的连接
对于同一网格内的所有特征点，两两之间建立双向边
确保不会创建自环（p1 != p2）

- 相邻网格间的连接：  
对于每个网格，检查其8个相邻网格（包括对角线方向）
如果相邻网格存在特征点，则与当前网格的特征点建立双向连接
这种设计方案的优点：
#### 这种设计方案的优点：
1. 保留了图像的局部特征信息（通过SIFT特征）
2. 利用空间位置信息构建图结构，保持了图像的拓扑关系
3. 通过网格划分控制图的复杂度
4. 双向边的设计增强了图的连通性

### 图数据保存
1. 将节点特征、边信息、节点 - 图像映射和标签保存为.npz文件
2. 分别保存训练集和测试集
3. 进行文件保存验证，确保数据完整性 

### 关键代码片段：
#### 1. 特征提取阶段：
```python
def extract_features(images):
    all_features = []
    node_image_map = []
    for img_idx, image in enumerate(images):
        # 提取SIFT特征
        features = extract_sift_features(image)
        # 生成空间位置信息
        spatial_info = np.array([[x, y] for y in range(32) for x in range(32)])
        if len(features) > 0:
            # 将SIFT特征与空间位置信息结合
            features = np.hstack([features, spatial_info[:len(features)]])
            all_features.extend(features)
            # 记录特征点对应的图像索引
            node_image_map.extend([img_idx] * len(features))
    return np.array(all_features, dtype=np.float32), np.array(node_image_map)

```
#### 2. 图结构构建阶段：
```python
def build_spatial_edges(features, image_size=32, grid_size=8):
    edges = []
    cell_size = image_size // grid_size
    
    # 将特征点分配到网格
    grid = {}
    for idx, (x, y) in enumerate(features[:, :2]):
        i, j = int(x // cell_size), int(y // cell_size)
        grid.setdefault((i, j), []).append(idx)
    
    # 构建边
    for (i, j), points in grid.items():
        # 同一网格内的连接
        for p1 in points:
            for p2 in points:
                if p1 != p2:
                    edges.extend([(p1, p2), (p2, p1)])
        
        # 相邻网格的连接
        for di in [-1, 0, 1]:
            for dj in [-1, 0, 1]:
                ni, nj = i + di, j + dj
                if (ni, nj) in grid:
                    for p1 in points:
                        for p2 in grid[(ni, nj)]:
                            edges.extend([(p1, p2), (p2, p1)])
    return edges
```
#### 3. 图数据保存阶段：
```python
# 保存训练数据
np.savez('train_sift_graph.npz',
         node_feat=train_node_feat,
         edges=np.column_stack([train_edge_src, train_edge_dst]),
         node_image_map=train_node_image_map,
         labels=np.array(train_labels))

# 保存测试数据
np.savez('test_sift_graph.npz',
         node_feat=test_node_feat,
         edges=np.column_stack([test_edge_src, test_edge_dst]),
         node_image_map=test_node_image_map,
         labels=np.array(test_labels))
```